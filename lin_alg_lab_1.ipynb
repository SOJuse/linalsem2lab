{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ee0cd9a-af04-4c30-a16f-fcfda7995078",
      "metadata": {
        "id": "4ee0cd9a-af04-4c30-a16f-fcfda7995078"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Автор:**\n",
        "Ларкин Григорий (446471) J3113"
      ],
      "metadata": {
        "id": "tJLoDkbXXK3-"
      },
      "id": "tJLoDkbXXK3-"
    },
    {
      "cell_type": "markdown",
      "id": "8375d4a7-4973-4006-b623-3e5438f0ae1a",
      "metadata": {
        "id": "8375d4a7-4973-4006-b623-3e5438f0ae1a"
      },
      "source": [
        "## **Теоретическая часть**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97b8788b-daf0-45fe-902d-cfa137ad9876",
      "metadata": {
        "id": "97b8788b-daf0-45fe-902d-cfa137ad9876"
      },
      "source": [
        "   PCA (Principal Component Analysis) — это метод снижения размерности данных, основанный на поиске новых ортогональных осей, которые максимизируют дисперсию проекции данных. Эти новые оси называются главными компонентами."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c31bfd3-7517-4051-9854-c7cfa2d322c2",
      "metadata": {
        "id": "7c31bfd3-7517-4051-9854-c7cfa2d322c2"
      },
      "source": [
        "PCA состоит из нескольких ключевых шагов:\n",
        "1. Центрирование данных\n",
        "3. Вычисление матрицы ковариаций\n",
        "4. Нахождение собственных значений и векторов\n",
        "5. Проекция данных"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b4441c-4d28-4e00-a805-f3a43627cd2d",
      "metadata": {
        "id": "68b4441c-4d28-4e00-a805-f3a43627cd2d"
      },
      "source": [
        "### Математическое обоснование **(Задание (Expert Level)):**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73e30a98-5c74-4dbc-8efa-0fdbefb73a87",
      "metadata": {
        "id": "73e30a98-5c74-4dbc-8efa-0fdbefb73a87"
      },
      "source": [
        "\n",
        "# Доказательство: Оптимальные направления PCA — собственные векторы матрицы ковариаций\n",
        "\n",
        "---\n",
        "\n",
        "## **Дано:**\n",
        "- Матрица данных:\n",
        "  $\\\n",
        "  X \\in \\mathbb{R}^{n \\times d}, \\quad \\text{где } n \\text{ — число наблюдений, } d \\text{ — число признаков}.\n",
        "  $\\\n",
        "  Данные **центрированы** (среднее каждого признака равно нулю).\n",
        "\n",
        "- **Матрица ковариаций**:\n",
        "\\$\n",
        "S = \\frac{1}{n} X^\\top X \\in \\mathbb{R}^{d \\times d}.\n",
        "\\$\n",
        "\n",
        "- **Оптимизационная задача**:  \n",
        "   Найти единичный вектор $\\mathbf{w} \\in \\mathbb{R}^d$, максимизирующий дисперсию проекций данных:  \n",
        "   $\\\n",
        "   \\max_{\\mathbf{w}} \\, \\mathbf{w}^\\top S \\mathbf{w}, \\quad \\text{при условии} \\quad \\mathbf{w}^\\top \\mathbf{w} = 1.\n",
        "   \\$\n",
        "\n",
        "(Мы ввели оптимизационную задачу, чтобы формально найти направление с максимальной дисперсией. Это помогает:\n",
        "\n",
        "1. **Свести задачу PCA к математической задаче**  \n",
        "   Максимизировать:  \n",
        "   \\$\n",
        "   \\mathbf{w}^\\top S \\mathbf{w}\n",
        "   \\$\n",
        "   При условии:  \n",
        "   \\$\n",
        "   \\|\\mathbf{w}\\| = 1\n",
        "   \\$\n",
        "\n",
        "2. **Использовать метод множителей Лагранжа**  \n",
        "   Решение автоматически приводит к уравнению:  \n",
        "   \\$\n",
        "   S\\mathbf{w} = \\lambda \\mathbf{w}\n",
        "   \\$\n",
        "   То есть к **собственным векторам**.\n",
        "\n",
        "3. **Доказать утверждение!!!**  \n",
        "   Решение задачи (собственные векторы) — это и есть **оптимальные направления PCA**.\n",
        "---\n",
        "\n",
        "## **Док-ть(используя оптимизационную задачу)**\n",
        "Оптимальные направления $\\mathbf{w}$, решающие задачу PCA, совпадают с **собственными векторами** матрицы ковариаций $S$, а соответствующие значения дисперсии — с её **собственными значениями**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Док-во(через метод множителей Лагранжа):**\n",
        "\n",
        "### **1. Метод множителей Лагранжа**\n",
        "\n",
        "**Функция Лагранжа**:  \n",
        "Вводится для учета ограничения:  \n",
        "$\n",
        "\\mathcal{L}(\\mathbf{w}, \\lambda) = \\underbrace{\\mathbf{w}^\\top S \\mathbf{w}}_{\\text{целевая функция}} - \\lambda \\underbrace{(\\mathbf{w}^\\top \\mathbf{w} - 1)}_{\\text{ограничение}},\n",
        "$\n",
        "где:\n",
        "- $\\lambda$ — множитель Лагранжа,\n",
        "- $\\mathbf{w}^\\top \\mathbf{w} - 1 = 0$ — условие нормировки.\n",
        "\n",
        "Нам нужно решить задачу условной оптимизации с ограничением $\\|\\mathbf{w}\\| = 1$.\n",
        "\n",
        "#### **Шаг 1: Нахождение стационарных точек**\n",
        "Для поиска экстремума вычисляем **градиент** функции Лагранжа по $\\mathbf{w}$ и приравниваем его к нулю:\n",
        "$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = 2S\\mathbf{w} - 2\\lambda \\mathbf{w} = 0.\n",
        "$\n",
        "Упрощаем:\n",
        "$\n",
        "S\\mathbf{w} = \\lambda \\mathbf{w}.\n",
        "$\n",
        "Это **уравнение на собственные значения**:  \n",
        "$\\mathbf{w}$ — собственный вектор матрицы \\(S\\),  \n",
        "$\\lambda$ — соответствующее собственное значение.\n",
        "\n",
        "#### **Шаг 2: Учет ограничения**\n",
        "Из условия нормировки $\\mathbf{w}^\\top \\mathbf{w} = 1$ следует, что:  \n",
        "$\n",
        "\\mathbf{w}^\\top S \\mathbf{w} = \\mathbf{w}^\\top (\\lambda \\mathbf{w}) = \\lambda \\underbrace{\\mathbf{w}^\\top \\mathbf{w}}_{=1} = \\lambda.\n",
        "$  \n",
        "Таким образом, **максимизация дисперсии** $\\mathbf{w}^\\top S \\mathbf{w}$ эквивалентна **максимизации** $\\lambda$.\n",
        "\n",
        "### **Почему нет других решений?**\n",
        "**Спектральная теорема**:  \n",
        "Так как $S$ симметрична, она имеет:\n",
        "- Ортонормированный базис из собственных векторов $ \\{\\mathbf{u}_1, \\mathbf{u}_2, ..., \\mathbf{u}_d\\} $,\n",
        "- Собственные значения $ \\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_d \\geq 0 $.\n",
        "\n",
        "**Любой вектор $ \\mathbf{v} $ можно разложить по этому базису**:  \n",
        "$\n",
        "\\mathbf{v} = \\sum_{i=1}^d c_i \\mathbf{u}_i, \\quad \\text{где } \\sum_{i=1}^d c_i^2 = 1.\n",
        "$\n",
        "\n",
        "**Дисперсия проекции**:  \n",
        "$\n",
        "\\mathbf{v}^\\top S \\mathbf{v} = \\sum_{i=1}^d \\lambda_i c_i^2.\n",
        "$\n",
        "\n",
        "Максимум достигается при $ c_1 = 1 $, $ c_2 = c_3 = ... = 0 $, то есть $\\mathbf{v} = \\mathbf{u}_1 $.\n",
        "\n",
        "### **Обобщение на несколько компонент**\n",
        "1. **Вторая компонента**:  \n",
        "   Ищется в подпространстве, ортогональном $ \\mathbf{u}_1 $:  \n",
        "   $\n",
        "   \\max_{\\mathbf{w} \\perp \\mathbf{u}_1} \\mathbf{w}^\\top S \\mathbf{w}, \\quad \\|\\mathbf{w}\\| = 1.\n",
        "   $\n",
        "\n",
        "   Решение — $\\mathbf{u}_2 $ (собственный вектор с $ \\lambda_2 $).\n",
        "\n",
        "2. **Ортогональность компонент**:  \n",
        "   Следует из ортогональности собственных векторов симметричной матрицы.\n",
        "\n",
        "---\n",
        "## **Итог**\n",
        "- **Главные компоненты PCA** — собственные векторы матрицы $S$, упорядоченные по убыванию $\\lambda$.\n",
        "- **Метод множителей Лагранжа** формально доказывает, что экстремумы достигаются на собственных векторах, а $\\lambda$ задаёт \"силу\" направления.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19b43bf-7246-4dc6-9535-db00819feafd",
      "metadata": {
        "id": "d19b43bf-7246-4dc6-9535-db00819feafd"
      },
      "source": [
        "## **Практическая часть**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PS: Правильность вычислений всех последующих заданий была проверенна через личный подсчет, а также онлайн калькуляторы матриц"
      ],
      "metadata": {
        "id": "rHVf5sERt8Bp"
      },
      "id": "rHVf5sERt8Bp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Класс SparseMatrixCRS из предыдущей лабораторной работы"
      ],
      "metadata": {
        "id": "KRwZGJ1WYTiP"
      },
      "id": "KRwZGJ1WYTiP"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b00969c7-2b37-4dc8-9cbc-ed21622ddd70",
      "metadata": {
        "id": "b00969c7-2b37-4dc8-9cbc-ed21622ddd70"
      },
      "outputs": [],
      "source": [
        "class SparseMatrixCRS:\n",
        "    \"\"\"\n",
        "    Класс для работы с разреженными матрицами в разряжённо-строчном формате (CRS).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n, m, values=None):\n",
        "        \"\"\"\n",
        "        Инициализирует размеры матрицы.\n",
        "        Если передан параметр `values`, то заполняет матрицу.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.values = []\n",
        "        self.column_indices = []\n",
        "        self.row_pointers = [0]\n",
        "\n",
        "        if values is not None:\n",
        "            for i, row in enumerate(values):\n",
        "                for j, value in enumerate(row):\n",
        "                    if value != 0:\n",
        "                        self.values.append(value)\n",
        "                        self.column_indices.append(j)\n",
        "                self.row_pointers.append(len(self.values))\n",
        "\n",
        "    def set_value(self, i, j, value):\n",
        "        \"\"\"\n",
        "        Устанавливает значение элемента матрицы (работает с ненулевыми элементами).\n",
        "        \"\"\"\n",
        "        if value == 0:\n",
        "            raise ValueError(\"CRS не поддерживает удаление элементов.\")\n",
        "\n",
        "        start = self.row_pointers[i]\n",
        "        end = self.row_pointers[i + 1]\n",
        "\n",
        "        for idx in range(start, end):\n",
        "            if self.column_indices[idx] == j:\n",
        "                self.values[idx] = value\n",
        "                return\n",
        "\n",
        "        self.values.insert(end, value)\n",
        "        self.column_indices.insert(end, j)\n",
        "\n",
        "        for k in range(i + 1, len(self.row_pointers)):\n",
        "            self.row_pointers[k] += 1\n",
        "\n",
        "    def get_value(self, i, j):\n",
        "        \"\"\"\n",
        "        Возвращает значение элемента матрицы (если элемент отсутствует, возвращает 0).\n",
        "        \"\"\"\n",
        "        start = self.row_pointers[i]\n",
        "        end = self.row_pointers[i + 1]\n",
        "\n",
        "        for idx in range(start, end):\n",
        "            if self.column_indices[idx] == j:\n",
        "                return self.values[idx]\n",
        "        return 0\n",
        "\n",
        "    def display(self):\n",
        "        \"\"\"\n",
        "        Возвращает матрицу в виде двумерного списка для отображения.\n",
        "        \"\"\"\n",
        "        result = [[0] * self.m for _ in range(self.n)]\n",
        "        for i in range(self.n):\n",
        "            start = self.row_pointers[i]\n",
        "            end = self.row_pointers[i + 1]\n",
        "            for idx in range(start, end):\n",
        "                j = self.column_indices[idx]\n",
        "                result[i][j] = self.values[idx]\n",
        "        return result\n",
        "\n",
        "    def trace(self):\n",
        "        \"\"\"\n",
        "        Вычисляет след матрицы (сумму элементов главной диагонали).\n",
        "        \"\"\"\n",
        "        if self.n != self.m:\n",
        "            raise ValueError(\"След определён только для квадратных матриц.\")\n",
        "        return sum(self.get_value(i, i) for i in range(self.n))\n",
        "\n",
        "\n",
        "def add_matrices(mat1, mat2):\n",
        "    \"\"\"\n",
        "    Сложение двух матриц в формате CRS.\n",
        "    \"\"\"\n",
        "    if mat1.n != mat2.n or mat1.m != mat2.m:\n",
        "        raise ValueError(\"Размеры матриц должны совпадать.\")\n",
        "\n",
        "    result = SparseMatrixCRS(mat1.n, mat1.m)\n",
        "\n",
        "    for i in range(mat1.n):\n",
        "        row_values = {}\n",
        "\n",
        "        start1, end1 = mat1.row_pointers[i], mat1.row_pointers[i + 1]\n",
        "        for idx in range(start1, end1):\n",
        "            col = mat1.column_indices[idx]\n",
        "            row_values[col] = mat1.values[idx]\n",
        "\n",
        "        start2, end2 = mat2.row_pointers[i], mat2.row_pointers[i + 1]\n",
        "        for idx in range(start2, end2):\n",
        "            col = mat2.column_indices[idx]\n",
        "            row_values[col] = row_values.get(col, 0) + mat2.values[idx]\n",
        "\n",
        "        for col, value in row_values.items():\n",
        "            if value != 0:\n",
        "                result.values.append(value)\n",
        "                result.column_indices.append(col)\n",
        "\n",
        "        result.row_pointers.append(len(result.values))\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49800f3e-6ec1-4be5-a829-ae7f99067b42",
      "metadata": {
        "id": "49800f3e-6ec1-4be5-a829-ae7f99067b42"
      },
      "source": [
        "### **Easy level**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание (Easy):** Реализовать метод Гаусса для решения СЛАУ\n",
        "\n",
        "\n",
        "$$\n",
        "A = \\begin{pmatrix}\n",
        "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
        "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{n1} & a_{n2} & \\cdots & a_{nn}\n",
        "\\end{pmatrix}, \\quad\n",
        "\\mathbf{b} = \\begin{pmatrix}\n",
        "b_1 \\\\\n",
        "b_2 \\\\\n",
        "\\vdots \\\\\n",
        "b_n\n",
        "\\end{pmatrix}, \\quad\n",
        "\\mathbf{x} = \\begin{pmatrix}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "\\vdots \\\\\n",
        "x_n\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "U6Y1OQoNZ6Fb"
      },
      "id": "U6Y1OQoNZ6Fb"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "7ed3f800-53ef-4683-9cab-53d1d7d31dab",
      "metadata": {
        "id": "7ed3f800-53ef-4683-9cab-53d1d7d31dab",
        "outputId": "59c6f6fa-8cb7-4613-9bdd-7261cddea64e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Решение СЛАУ:\n",
            "x1 = 9.999999999999995\n",
            "x2 = 12.499999999999996\n",
            "x3 = 15.499999999999993\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "def gauss_solver(A: SparseMatrixCRS, b: SparseMatrixCRS) -> List[SparseMatrixCRS]:\n",
        "    \"\"\"\n",
        "    Решает систему линейных уравнений Ax = b методом Гаусса.\n",
        "    \"\"\"\n",
        "\n",
        "    n = A.n\n",
        "\n",
        "    Ab = [A.display()[i] + [b.get_value(i, 0)] for i in range(n)]\n",
        "\n",
        "    for i in range(n):\n",
        "        max_row = max(range(i, n), key=lambda r: abs(Ab[r][i]))\n",
        "        if Ab[max_row][i] == 0:\n",
        "            raise ValueError(\"Система несовместна\")\n",
        "        Ab[i], Ab[max_row] = Ab[max_row], Ab[i]\n",
        "\n",
        "        pivot = Ab[i][i]\n",
        "        Ab[i] = [x / pivot for x in Ab[i]]\n",
        "\n",
        "        for j in range(i + 1, n):\n",
        "            factor = Ab[j][i]\n",
        "            Ab[j] = [a - factor * b for a, b in zip(Ab[j], Ab[i])]\n",
        "\n",
        "    x = [0 for _ in range(n)]\n",
        "    for i in reversed(range(n)):\n",
        "        x[i] = Ab[i][-1] - sum(Ab[i][j] * x[j] for j in range(i + 1, n))\n",
        "\n",
        "    return [SparseMatrixCRS(1, 1, [[xi]]) for xi in x]\n",
        "\n",
        "\n",
        "A = SparseMatrixCRS(3, 3, [\n",
        "    [-6.0, -2.0, 6.0],\n",
        "    [-2.0, -3.0, 3.0],\n",
        "    [-4.0, -2.0, 4.0]\n",
        "])\n",
        "b_data = [\n",
        "    [8],\n",
        "    [-11],\n",
        "    [-3]\n",
        "]\n",
        "\n",
        "\n",
        "b = SparseMatrixCRS(3, 1, b_data)\n",
        "\n",
        "solution = gauss_solver(A, b)\n",
        "print(\"Решение СЛАУ:\")\n",
        "for i, xi in enumerate(solution):\n",
        "    print(f\"x{i+1} =\", xi.get_value(0, 0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e246e0c2-bcbb-4ce6-b10a-d6e34d36456c",
      "metadata": {
        "id": "e246e0c2-bcbb-4ce6-b10a-d6e34d36456c"
      },
      "source": [
        "**Задание (Easy):** Реализовать функцию центрирования данных:\n",
        "$$\n",
        "X_{\\text{centered}} = X - \\overline{X}\n",
        "$$\n",
        "где $\\overline{X}$ — матрица средних значений по каждому признаку."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6f273fdb-b385-48f2-a0e8-4a21c85f5145",
      "metadata": {
        "id": "6f273fdb-b385-48f2-a0e8-4a21c85f5145",
        "outputId": "89177379-0251-4897-9962-1f6ebc28a06f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Центрированные данные:\n",
            "[-2.0, 0.3333333333333335, 1.666666666666667]\n",
            "[2.0, -0.6666666666666665, -1.333333333333333]\n",
            "[0, 0.3333333333333335, -0.33333333333333304]\n"
          ]
        }
      ],
      "source": [
        "def center_data(X: SparseMatrixCRS) -> SparseMatrixCRS:\n",
        "    \"\"\"\n",
        "    Центрирует матрицу признаков по каждому столбцу.\n",
        "    \"\"\"\n",
        "\n",
        "    n, m = X.n, X.m\n",
        "    raw = X.display()\n",
        "\n",
        "    means = [sum(raw[i][j] for i in range(n)) / n for j in range(m)]\n",
        "    centered = [\n",
        "        [raw[i][j] - means[j] for j in range(m)]\n",
        "        for i in range(n)\n",
        "    ]\n",
        "    return SparseMatrixCRS(n, m, centered)\n",
        "\n",
        "X = SparseMatrixCRS(3, 3, [\n",
        "    [-6.0, -2.0, 6.0],\n",
        "    [-2.0, -3.0, 3.0],\n",
        "    [-4.0, -2.0, 4.0]\n",
        "])\n",
        "Xc = center_data(X)\n",
        "print(\"Центрированные данные:\")\n",
        "for row in Xc.display():\n",
        "    print(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание (Normal):** Найти собственные значения матрицы\n",
        "C методом бисекции:\n",
        "$$\n",
        "\\det(C - \\lambda I) = 0\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $C$ — матрица ковариации ($m \\times m$)\n",
        "- $\\lambda$ — собственное значение\n",
        "- $I$ — единичная матрица"
      ],
      "metadata": {
        "id": "KEdbF7RhaEgU"
      },
      "id": "KEdbF7RhaEgU"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_determinant(matrix: List[List[float]]) -> float:\n",
        "    \"\"\"\n",
        "    Вычисляет определитель квадратной матрицы через разложение по строке.\n",
        "    \"\"\"\n",
        "    n = len(matrix)\n",
        "    if n == 1:\n",
        "        return matrix[0][0]\n",
        "    if n == 2:\n",
        "        return matrix[0][0]*matrix[1][1] - matrix[0][1]*matrix[1][0]\n",
        "\n",
        "    det = 0\n",
        "    for col in range(n):\n",
        "        submatrix = [\n",
        "            [matrix[i][j] for j in range(n) if j != col]\n",
        "            for i in range(1, n)\n",
        "        ]\n",
        "        det += ((-1) ** col) * matrix[0][col] * get_determinant(submatrix)\n",
        "    return det\n",
        "\n",
        "\n",
        "def find_eigenvalues(C: SparseMatrixCRS, tol: float = 1e-6, a: float = -10, b: float = 10) -> List[float]:\n",
        "    \"\"\"\n",
        "    Ищет собственные значения методом бисекции по определителю.\n",
        "    \"\"\"\n",
        "    from copy import deepcopy\n",
        "    n = C.n\n",
        "    raw_C = C.display()\n",
        "    eigenvalues = []\n",
        "\n",
        "    def f(lam):\n",
        "        M = [[raw_C[i][j] - (lam if i == j else 0) for j in range(n)] for i in range(n)]\n",
        "        return get_determinant(M)\n",
        "\n",
        "    intervals = []\n",
        "    steps = 1000\n",
        "    step_size = (b - a) / steps\n",
        "\n",
        "    for i in range(steps):\n",
        "        x0 = a + i * step_size\n",
        "        x1 = x0 + step_size\n",
        "        if f(x0) * f(x1) < 0:\n",
        "            intervals.append((x0, x1))\n",
        "\n",
        "    for left, right in intervals:\n",
        "        for _ in range(100):\n",
        "            mid = (left + right) / 2\n",
        "            f_mid = f(mid)\n",
        "            if abs(f_mid) < tol:\n",
        "                break\n",
        "            if f(left) * f_mid < 0:\n",
        "                right = mid\n",
        "            else:\n",
        "                left = mid\n",
        "        root = round((left + right) / 2, 6)\n",
        "        if all(abs(root - ev) > tol for ev in eigenvalues):\n",
        "            eigenvalues.append(root)\n",
        "\n",
        "    return sorted(eigenvalues)\n",
        "\n",
        "С = SparseMatrixCRS(3, 3, [\n",
        "    [-6.0, -2.0, 6.0],\n",
        "    [-2.0, -3.0, 3.0],\n",
        "    [-4.0, -2.0, 6.0]\n",
        "])\n",
        "\n",
        "eigenvalues = find_eigenvalues(С, tol=1e-6)\n",
        "print(\"Собственные значения:\")\n",
        "for i, val in enumerate(eigenvalues, 1):\n",
        "    print(f\"λ{i} = {val:.6f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eB9U4sxJacFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ea9acd6-e087-4e1e-fd1e-8766850b9896"
      },
      "id": "eB9U4sxJacFJ",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Собственные значения:\n",
            "λ1 = -4.000000\n",
            "λ2 = -2.000000\n",
            "λ3 = 3.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание (Normal):** Найти собственные векторы матрицы $C$:\n",
        "\n",
        "$(C − λI)v = 0$"
      ],
      "metadata": {
        "id": "eg8qhBNhuYna"
      },
      "id": "eg8qhBNhuYna"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def normalize_vector(vec: List[float]) -> List[float]:\n",
        "    norm = math.sqrt(sum(x ** 2 for x in vec))\n",
        "    return [x / norm for x in vec] if norm != 0 else vec\n",
        "\n",
        "def find_eigenvectors(C: SparseMatrixCRS, eigenvalues: List[float]) -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Решает (C - λI)v = 0 для каждого λ, возвращает нормализованные собственные векторы.\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    n = C.n\n",
        "    raw_C = C.display()\n",
        "\n",
        "    for lam in eigenvalues:\n",
        "        A_lam = [\n",
        "            [raw_C[i][j] - (lam if i == j else 0.0) for j in range(n)]\n",
        "            for i in range(n)\n",
        "        ]\n",
        "\n",
        "\n",
        "        b = [[1.0] if i == 0 else [0.0] for i in range(n)]\n",
        "        try:\n",
        "            A_lam_sparse = SparseMatrixCRS(n, n, A_lam)\n",
        "            b_sparse = SparseMatrixCRS(n, 1, b)\n",
        "            solution_sparse = gauss_solver(A_lam_sparse, b_sparse)\n",
        "            vector = [x.get_value(0, 0) for x in solution_sparse]\n",
        "            vector = normalize_vector(vector)\n",
        "            vectors.append(vector)\n",
        "        except Exception as e:\n",
        "            vectors.append([0.0] * n)\n",
        "            print(f\"Ошибка при нахождении вектора для λ = {lam}: {e}\")\n",
        "\n",
        "    return vectors\n",
        "eigenvalues = find_eigenvalues(С)\n",
        "eigenvectors = find_eigenvectors(С, eigenvalues)\n",
        "\n",
        "print(\"Собственные значения и соответствующие нормализованные векторы:\")\n",
        "for i, (lam, vec) in enumerate(zip(eigenvalues, eigenvectors), 1):\n",
        "    print(f\"λ{i} = {lam:.6f}, v = [{', '.join(f'{x:.4f}' for x in vec)}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqz71UGemriQ",
        "outputId": "873377ea-f7fe-49e4-aa69-8b4a2909b426"
      },
      "id": "lqz71UGemriQ",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ошибка при нахождении вектора для λ = -4.0: Система несовместна\n",
            "Ошибка при нахождении вектора для λ = -2.0: Система несовместна\n",
            "Собственные значения и соответствующие нормализованные векторы:\n",
            "λ1 = -4.000000, v = [0.0000, 0.0000, 0.0000]\n",
            "λ2 = -2.000000, v = [0.0000, 0.0000, 0.0000]\n",
            "λ3 = 3.000000, v = [-0.4983, -0.2491, -0.8305]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание (Normal): Вычислить долю объяснённой дисперсии:\n",
        "\n",
        "$\n",
        "\\gamma = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^m \\lambda_i},\n",
        "$\n",
        "\n",
        "где:\n",
        "- $ \\lambda_i $ — собственные значения матрицы ковариаций $ S $, соответствующие главным компонентам,\n",
        "- $ m $ — общее число главных компонент (равно числу признаков $ d $),\n",
        "- $ k $ — число рассматриваемых компонент ($ k \\leq m $).\n",
        "\n",
        "**Пояснение формулы**\n",
        "\n",
        "1. **Числитель** $( \\sum_{i=1}^k \\lambda_i $):  \n",
        "   Сумма собственных значений для первых $ k $ компонент.  \n",
        "   *(Общая дисперсия, \"объяснённая\" выбранными компонентами.)*\n",
        "\n",
        "2. **Знаменатель** $( \\sum_{i=1}^m \\lambda_i $):  \n",
        "   Сумма всех собственных значений.  \n",
        "   *(Полная дисперсия данных (по всем компонентам))*\n",
        "\n",
        "3. **Результат** $( \\gamma )$:  \n",
        "   Доля дисперсии, сохранённая при использовании $ k $ компонент.  \n",
        "   *(Например, $ \\gamma = 0.95 $ означает, что 95% информации сохранено)*\n"
      ],
      "metadata": {
        "id": "TE8T5voLuKo-"
      },
      "id": "TE8T5voLuKo-"
    },
    {
      "cell_type": "code",
      "source": [
        "def explained_variance_ratio(eigenvalues: List[float], k: int) -> float:\n",
        "    \"\"\"\n",
        "    Вычисляет долю объяснённой дисперсии с помощью первых k наибольших λ.\n",
        "    \"\"\"\n",
        "    if not eigenvalues:\n",
        "        raise ValueError(\"Список собственных значений пуст.\")\n",
        "    if k <= 0 or k > len(eigenvalues):\n",
        "        raise ValueError(\"Некорректное значение k.\")\n",
        "\n",
        "    sorted_values = sorted(eigenvalues, reverse=True)\n",
        "    total_variance = sum(sorted_values)\n",
        "    explained_variance = sum(sorted_values[:k])\n",
        "    return explained_variance / total_variance if total_variance != 0 else 0.0\n",
        "eigenvalues = find_eigenvalues(С)\n",
        "gamma = explained_variance_ratio(eigenvalues, k=3)\n",
        "print(f\"Доля объяснённой дисперсии для k=3: {gamma:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rPxeX1IcvbC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e6bf12-c014-4542-ec28-60017d795bb8"
      },
      "id": "rPxeX1IcvbC9",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Доля объяснённой дисперсии для k=3: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hard Level\n"
      ],
      "metadata": {
        "id": "gNo9860uWpGq"
      },
      "id": "gNo9860uWpGq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание (Hard):** Реализовать полный алгоритм PCA:\n",
        "1. Центрирование данных.\n",
        "2. Вычисление матрицы выборочных ковариаций.\n",
        "3. Нахождение собственных значений и векторов.\n",
        "4. Проекция данных на главные компоненты.\n"
      ],
      "metadata": {
        "id": "-T_7XM-7EoMj"
      },
      "id": "-T_7XM-7EoMj"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "def project_data(X, components):\n",
        "    \"\"\"\n",
        "    Проецирует данные X на заданные компоненты.\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    k = len(components)\n",
        "    X_proj = []\n",
        "    for row in X:\n",
        "        proj_row = []\n",
        "        for comp in components:\n",
        "            dot = sum(x * c for x, c in zip(row, comp))\n",
        "            proj_row.append(dot)\n",
        "        X_proj.append(proj_row)\n",
        "    return X_proj\n",
        "\n",
        "def pca(X: 'Matrix', k: int) -> Tuple['Matrix', float]:\n",
        "    \"\"\"\n",
        "    Основной алгоритм PCA (метод главных компонент).\n",
        "\n",
        "    Вход:\n",
        "        X — матрица признаков (список списков или SparseMatrixCRS)\n",
        "        k — количество главных компонент\n",
        "\n",
        "    Выход:\n",
        "        X_proj — проекция данных на k компонент\n",
        "        gamma — доля объяснённой дисперсии (сумма первых k собственных значений /\n",
        "                 сумма всех собственных значений)\n",
        "    \"\"\"\n",
        "    if not isinstance(X, SparseMatrixCRS):\n",
        "        n = len(X)\n",
        "        m = len(X[0])\n",
        "        X = SparseMatrixCRS(n, m, X)\n",
        "\n",
        "    X_centered = center_data(X)\n",
        "\n",
        "    C = covariance_matrix(X_centered)\n",
        "\n",
        "    eigenvalues = find_eigenvalues(C)\n",
        "    eigenvectors = find_eigenvectors(C, eigenvalues)\n",
        "\n",
        "    eig_pairs = sorted(zip(eigenvalues, eigenvectors), key=lambda x: x[0], reverse=True)\n",
        "    top_components = [vec for _, vec in eig_pairs[:k]]\n",
        "\n",
        "    X_proj = project_data(X_centered.display(), top_components)\n",
        "\n",
        "    gamma = explained_variance_ratio(eigenvalues, k)\n",
        "\n",
        "    return X_proj, gamma\n",
        "\n",
        "proj, gamma = pca(X, k=2)\n",
        "\n",
        "print(\"Проекция на 2 главные компоненты:\")\n",
        "for row in proj:\n",
        "    print([\"{:.4f}\".format(x) for x in row])\n",
        "\n",
        "print(f\"\\nДоля объяснённой дисперсии: {gamma:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "GIINtnG286k3",
        "outputId": "74a7be48-cce5-4018-bed1-eae947e722c7"
      },
      "id": "GIINtnG286k3",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'covariance_matrix' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-ef5f1a553e9c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_proj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mproj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Проекция на 2 главные компоненты:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-ef5f1a553e9c>\u001b[0m in \u001b[0;36mpca\u001b[0;34m(X, k)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mX_centered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcenter_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_centered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0meigenvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_eigenvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'covariance_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание (Hard):** Визуализировать проекцию данных на первые две главные компоненты. Задание является единственным исключением из правил, в котором можно\n",
        "пользоваться сторонними модулями для построения\n",
        "графиков. Рекомендуется Matplotlib."
      ],
      "metadata": {
        "id": "f65mj1lbKx5D"
      },
      "id": "f65mj1lbKx5D"
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.figure import Figure\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def reconstruction_error(X_orig: 'Matrix', X_recon: 'Matrix') -> float:\n",
        "  \"\"\"\n",
        "  Вычисляет среднеквадратичную ошибку восстановления (MSE) между исходными и восстановленными данными.\n",
        "\n",
        "  Вход:\n",
        "  X_orig: исходные данные (n×m)\n",
        "  X_recon: восстановленные данные (n×m)\n",
        "  Выход: среднеквадратическая ошибка MSE\n",
        "  \"\"\"\n",
        "  n = len(X_orig)\n",
        "  m = len(X_orig[0])\n",
        "  total_error = 0.0\n",
        "  for i in range(n):\n",
        "    for j in range(m):\n",
        "        diff = X_orig[i][j] - X_recon[i][j]\n",
        "        total_error += diff ** 2\n",
        "\n",
        "  mse = total_error / (n * m)\n",
        "  return mse\n",
        "\n",
        "def generate_colors(n):\n",
        "    random.seed(42)\n",
        "    return [(random.random(), random.random(), random.random()) for _ in range(n)]\n",
        "\n",
        "def plot_comparison(X_data, X_proj, colors):\n",
        "    \"\"\"\n",
        "    Строит графики сравнения исходных данных и проекции после PCA.\n",
        "\n",
        "    Вход:\n",
        "        X_data — оригинальные данные в исходном пространстве\n",
        "        X_proj — данные после проекции на главные компоненты\n",
        "        colors — список цветов для отображения каждого объекта\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "\n",
        "    x_orig = [row[0] for row in X_data]\n",
        "    y_orig = [row[1] for row in X_data]\n",
        "    for i in range(len(X_data)):\n",
        "        axs[0].scatter(x_orig[i], y_orig[i], color=colors[i], s=60)\n",
        "        axs[0].text(x_orig[i] + 0.02, y_orig[i], str(i), fontsize=9)\n",
        "\n",
        "    axs[0].set_title(\"Исходные данные (до PCA)\")\n",
        "    axs[0].set_xlabel(\"Признак 1\")\n",
        "    axs[0].set_ylabel(\"Признак 2\")\n",
        "    axs[0].grid(True)\n",
        "    axs[0].set_aspect('equal', adjustable='box')\n",
        "\n",
        "    x_proj = [row[0] for row in X_proj]\n",
        "    y_proj = [row[1] for row in X_proj]\n",
        "    for i in range(len(X_proj)):\n",
        "        axs[1].scatter(x_proj[i], y_proj[i], color=colors[i], s=60)\n",
        "        axs[1].text(x_proj[i] + 0.02, y_proj[i], str(i), fontsize=9)\n",
        "\n",
        "    axs[1].set_title(\"PCA: проекция на первые 2 компоненты\")\n",
        "    axs[1].set_xlabel(\"PC1\")\n",
        "    axs[1].set_ylabel(\"PC2\")\n",
        "    axs[1].grid(True)\n",
        "    axs[1].set_aspect('equal', adjustable='box')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "X_data = [\n",
        "    [2.5, 2.4, 0.5],\n",
        "    [0.5, 0.7, -0.3],\n",
        "    [2.2, 2.9, 0.8],\n",
        "    [1.9, 2.2, 0.4],\n",
        "    [3.1, 3.0, 1.1],\n",
        "    [2.3, 2.7, 0.9],\n",
        "    [2.0, 1.6, 0.2],\n",
        "    [1.0, 1.1, -0.2],\n",
        "    [1.5, 1.6, 0.1],\n",
        "    [1.1, 0.9, -0.4],\n",
        "    [3.2, 3.1, 1.2],\n",
        "    [0.6, 0.8, -0.2],\n",
        "    [2.4, 2.6, 0.7],\n",
        "    [2.1, 2.0, 0.3],\n",
        "    [0.9, 1.0, -0.1],\n",
        "    [3.0, 2.9, 1.0],\n",
        "    [2.7, 2.8, 0.9],\n",
        "    [2.8, 2.5, 0.6],\n",
        "    [1.4, 1.5, 0.0],\n",
        "    [1.2, 1.3, -0.2]\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "colors = generate_colors(len(X_data))\n",
        "\n",
        "X = SparseMatrixCRS(n=len(X_data), m=3, values=X_data)\n",
        "X_proj, gamma = pca(X, k=2)\n",
        "\n",
        "plot_comparison(X_data, X_proj, colors)\n"
      ],
      "metadata": {
        "id": "xkFwiZRM9TsR"
      },
      "id": "xkFwiZRM9TsR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание (Hard):** Вычислить среднеквадратическую ошибку\n",
        "восстановления данных\n",
        "\n",
        "Среднеквадратическая ошибка (Mean Squared Error) вычисляется по формуле:\n",
        "\n",
        "$\n",
        "MSE = \\frac{1}{n \\cdot m} \\sum_{i=1}^{n} \\sum_{j=1}^{m} (X_{\\text{orig}}^{(i,j)} - X_{\\text{recon}}^{(i,j)})^2\n",
        "$\n",
        "\n",
        "где:\n",
        "- $ X_{\\text{orig}} $ — исходная матрица данных (размер $ n \\times m $),\n",
        "- $ X_{\\text{recon}} $ — восстановленная матрица после PCA (размер $ n \\times m $),\n",
        "- $ n $ — количество наблюдений (строк),\n",
        "- $ m $ — количество признаков (столбцов).\n",
        "\n",
        "---\n",
        "1. **Разность матриц**:  \n",
        "   $\n",
        "   X_{\\text{orig}} - X_{\\text{recon}}\n",
        "   $\n",
        "\n",
        "   Поэлементно вычитаем восстановленные данные из исходных. Это дает матрицу ошибок.\n",
        "\n",
        "2. **Возведение в квадрат**:  \n",
        "   $\n",
        "   (X_{\\text{orig}}^{(i,j)} - X_{\\text{recon}}^{(i,j)})^2\n",
        "   $\n",
        "\n",
        "   Квадратирование устраняет знаки ошибок и усиливает влияние больших отклонений.\n",
        "\n",
        "3. **Суммирование**:  \n",
        "   $\n",
        "   \\sum_{i,j}\n",
        "   $  \n",
        "   Сумма всех квадратов ошибок по всем элементам матрицы.\n",
        "\n",
        "4. **Нормировка**:  \n",
        "   $\n",
        "   \\frac{1}{n \\cdot m}\n",
        "   $  \n",
        "   Деление на общее количество элементов (\\( n \\times m \\)) дает среднее значение ошибки.\n",
        "---\n",
        "- **MSE = 0**: Восстановление идеальное (данные полностью сохранены).\n",
        "- **MSE > 0**: Чем меньше MSE, тем лучше качество восстановления.\n",
        "- **Единицы измерения**: MSE измеряется в квадратах единиц исходных данных (например, если данные в метрах, то MSE будет в м²).\n"
      ],
      "metadata": {
        "id": "qVielSBjRAbN"
      },
      "id": "qVielSBjRAbN"
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_data(X_proj, components):\n",
        "    \"\"\"\n",
        "    Восстанавливает данные из проекции: X_proj × components.\n",
        "    X_proj — (n×k), components — (k×m)\n",
        "    Возвращает X_recon — (n×m)\n",
        "    \"\"\"\n",
        "    n = len(X_proj)\n",
        "    k = len(components)\n",
        "    m = len(components[0])\n",
        "\n",
        "    components_T = [[components[j][i] for j in range(k)] for i in range(m)]\n",
        "\n",
        "    X_recon = []\n",
        "    for i in range(n):\n",
        "        row = []\n",
        "        for j in range(m):\n",
        "            value = sum(X_proj[i][t] * components_T[j][t] for t in range(k))\n",
        "            row.append(value)\n",
        "        X_recon.append(row)\n",
        "\n",
        "    return X_recon\n",
        "\n",
        "def reconstruction_error(X_orig, X_recon):\n",
        "    \"\"\"\n",
        "    Среднеквадратическая ошибка восстановления (MSE).\n",
        "    \"\"\"\n",
        "    n = len(X_orig)\n",
        "    m = len(X_orig[0])\n",
        "    total_error = 0.0\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            diff = X_orig[i][j] - X_recon[i][j]\n",
        "            total_error += diff ** 2\n",
        "\n",
        "    return total_error / (n * m)\n",
        "\n",
        "X_proj, gamma = pca(X_data, k=2)\n",
        "\n",
        "X_crs = SparseMatrixCRS(n=len(X_data), m=3, values=X_data)\n",
        "X_centered = center_data(X_crs)\n",
        "C = covariance_matrix(X_centered)\n",
        "eigenvalues = find_eigenvalues(C)\n",
        "eigenvectors = find_eigenvectors(C, eigenvalues)\n",
        "eig_pairs = sorted(zip(eigenvalues, eigenvectors), key=lambda x: x[0], reverse=True)\n",
        "top_components = [vec for _, vec in eig_pairs[:2]]\n",
        "\n",
        "X_recon = reconstruct_data(X_proj, top_components)\n",
        "mse = reconstruction_error(X_centered.display(), X_recon)\n",
        "\n",
        "print(f\"Доля объяснённой дисперсии: {gamma:.4f}\")\n",
        "print(f\"Среднеквадратическая ошибка восстановления: {mse:.6f}\")"
      ],
      "metadata": {
        "id": "1xJCtvzVSWoY"
      },
      "id": "1xJCtvzVSWoY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expert Level\n"
      ],
      "metadata": {
        "id": "Phg4UjorW169"
      },
      "id": "Phg4UjorW169"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание (Expert):** Добавить автоматический выбор числа\n",
        "главных компонент на основе порога объяснённой дисперсии\n",
        "(встроить это в реализованную функцию pca):\n",
        "\n",
        "$$\n",
        "k = \\min \\left\\{ k : \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{m} \\lambda_i} \\geq \\text{threshold} \\right\\}.\n",
        "$$\n",
        "\n",
        "Эта формула используется для выбора минимального количества главных компонент\n",
        "$ k $, которые сохраняют заданную долю дисперсии (например, 90%).\n",
        "\n",
        "- $ \\lambda_i $ — собственные значения (дисперсии) компонент, отсортированные по убыванию.\n",
        "- $ m $ — общее число компонент.\n",
        "- $ \\text{threshold} $ — пороговая доля (например, 0.9 для 90%).\n",
        "\n",
        "Формула выбирает наименьшее \\( k \\), такое что сумма первых \\( k \\) собственных значений составляет не менее заданного порога от общей суммы.\n"
      ],
      "metadata": {
        "id": "z94EEQazxUyJ"
      },
      "id": "z94EEQazxUyJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_select_k(eigenvalues, threshold=0.95):\n",
        "    \"\"\"\n",
        "    Автоматический выбор числа компонент по суммарной дисперсии.\n",
        "    \"\"\"\n",
        "    total = sum(eigenvalues)\n",
        "    cumulative = 0.0\n",
        "\n",
        "    for i, val in enumerate(eigenvalues):\n",
        "        cumulative += val\n",
        "        if cumulative / total >= threshold:\n",
        "            return i + 1\n",
        "\n",
        "    return len(eigenvalues)\n"
      ],
      "metadata": {
        "id": "cc63oqDvTjNS"
      },
      "id": "cc63oqDvTjNS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_missing_values(X):\n",
        "    \"\"\"\n",
        "    Заменяет NaN в матрице X средним по столбцу.\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    m = len(X[0])\n",
        "\n",
        "    col_means = []\n",
        "    for j in range(m):\n",
        "        values = [X[i][j] for i in range(n) if not math.isnan(X[i][j])]\n",
        "        if values:\n",
        "            mean_j = sum(values) / len(values)\n",
        "        else:\n",
        "            mean_j = 0.0\n",
        "        col_means.append(mean_j)\n",
        "\n",
        "    X_filled = []\n",
        "    for i in range(n):\n",
        "        row = []\n",
        "        for j in range(m):\n",
        "            val = X[i][j]\n",
        "            row.append(col_means[j] if math.isnan(val) else val)\n",
        "        X_filled.append(row)\n",
        "\n",
        "    return X_filled\n"
      ],
      "metadata": {
        "id": "NdHnBYdVXxfs"
      },
      "id": "NdHnBYdVXxfs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def pca_new(X: 'Matrix', k: int = -1, auto: bool = False, threshold: float = 0.95):\n",
        "    \"\"\"\n",
        "    PCA с обработкой пропусков и авто-выбором числа компонент.\n",
        "\n",
        "    Вход:\n",
        "        X — матрица данных (список списков или SparseMatrixCRS)\n",
        "        k — число компонент (используется, если auto=False)\n",
        "        auto — если True, выбрать k автоматически по threshold\n",
        "        threshold — порог объяснённой дисперсии\n",
        "\n",
        "    Выход:\n",
        "        X_proj — проекция на k компонент\n",
        "        gamma — доля объяснённой дисперсии\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(X, SparseMatrixCRS):\n",
        "        X = handle_missing_values(X)\n",
        "        n = len(X)\n",
        "        m = len(X[0])\n",
        "        X = SparseMatrixCRS(n, m, X)\n",
        "\n",
        "    X_centered = center_data(X)\n",
        "\n",
        "    C = covariance_matrix(X_centered)\n",
        "\n",
        "    eigenvalues = find_eigenvalues(C)\n",
        "    eigenvectors = find_eigenvectors(C, eigenvalues)\n",
        "\n",
        "    eig_pairs = sorted(zip(eigenvalues, eigenvectors), key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    if auto:\n",
        "        k = auto_select_k([val for val, _ in eig_pairs], threshold)\n",
        "\n",
        "    top_components = [vec for _, vec in eig_pairs[:k]]\n",
        "\n",
        "    X_proj = project_data(X_centered.display(), top_components)\n",
        "\n",
        "    gamma = explained_variance_ratio([val for val, _ in eig_pairs], k)\n",
        "\n",
        "    return X_proj, gamma\n",
        "data_with_nans = [\n",
        "    [2.5, 2.4, float('nan')],\n",
        "    [0.5, float('nan'), -0.3],\n",
        "    [2.2, 2.9, 0.8],\n",
        "    [float('nan'), 2.2, 0.4],\n",
        "]\n",
        "\n",
        "X_proj, gamma = pca_new(data_with_nans, auto=True)\n",
        "\n",
        "print(\"Проекция:\")\n",
        "for row in X_proj:\n",
        "    print(row)\n",
        "\n",
        "print(f\"Объяснённая дисперсия: {gamma:.4f}\")\n"
      ],
      "metadata": {
        "id": "KU4mDSbcXzZ5"
      },
      "id": "KU4mDSbcXzZ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание (Expert):** Обработать пропущенные значения в данных:\n",
        "\n",
        "$$\n",
        "X_{\\text{filled}} =\n",
        "\\begin{cases}\n",
        "X_{ij}, & \\text{если } X_{ij} \\ne \\text{NaN}, \\\\\n",
        "\\text{mean}(X_j), & \\text{иначе}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Формула описывает стратегию обработки пропущенных значений (NaN) в таблице данных:\n",
        "\n",
        "- Если значение в ячейке $ X_{ij} $ **не является пропущенным** (то есть $ X_{ij} \\ne \\text{NaN} $), то оно сохраняется как есть.\n",
        "- Если значение **пропущено** $( X_{ij} = \\text{NaN} $), то оно заменяется **средним значением по столбцу** $ X_j $.\n",
        "\n",
        "Это типичная стратегия заполнения пропусков с помощью среднего значения по признаку.\n"
      ],
      "metadata": {
        "id": "iXQQOsya9PfT"
      },
      "id": "iXQQOsya9PfT"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def add_noise_and_compare(X, noise_level=0.1):\n",
        "    \"\"\"\n",
        "    Добавляет шум к данным и сравнивает результаты PCA.\n",
        "\n",
        "    Вход:\n",
        "        X — матрица данных (обычный список списков)\n",
        "        noise_level — доля от std (например, 0.1 означает 10% шума)\n",
        "\n",
        "    Выход:\n",
        "        Печатает сравнение PCA до и после шума\n",
        "    \"\"\"\n",
        "\n",
        "    X_clean = [row[:] for row in X]\n",
        "\n",
        "    n = len(X)\n",
        "    m = len(X[0])\n",
        "    means = [sum(X[i][j] for i in range(n)) / n for j in range(m)]\n",
        "    stds = []\n",
        "    for j in range(m):\n",
        "        variance = sum((X[i][j] - means[j]) ** 2 for i in range(n)) / n\n",
        "        stds.append(variance ** 0.5)\n",
        "\n",
        "    X_noisy = []\n",
        "    for i in range(n):\n",
        "        row = []\n",
        "        for j in range(m):\n",
        "            noise = random.gauss(0, stds[j] * noise_level)\n",
        "            row.append(X[i][j] + noise)\n",
        "        X_noisy.append(row)\n",
        "\n",
        "    proj_clean, gamma_clean = pca_new(X_clean, auto=True)\n",
        "\n",
        "    proj_noisy, gamma_noisy = pca_new(X_noisy, auto=True)\n",
        "\n",
        "    print(\"До шума:\")\n",
        "    print(f\"  Объяснённая дисперсия: {gamma_clean:.4f}\")\n",
        "    print(\"После шума:\")\n",
        "    print(f\"  Объяснённая дисперсия: {gamma_noisy:.4f}\")\n",
        "X_data = [\n",
        "    [2.5, 2.4, 0.5],\n",
        "    [0.5, 0.7, -0.3],\n",
        "    [2.2, 2.9, 0.8],\n",
        "    [1.9, 2.2, 0.4],\n",
        "    [3.1, 3.0, 1.1],\n",
        "]\n",
        "\n",
        "add_noise_and_compare(X_data, noise_level=0.1)\n"
      ],
      "metadata": {
        "id": "3TlvueuxaO9T"
      },
      "id": "3TlvueuxaO9T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание (Expert):** Исследовать влияние шума на PCA:\n",
        "\n",
        "- Добавить случайный шум к данным.\n",
        "- **Сравнить** результаты PCA до и после добавления шума.\n",
        "\n",
        "**Задание (Expert):** Применить PCA к реальному датасету:\n",
        "\n",
        "- Загрузить данныеa.\n",
        "- Сравнить метрики качества до и после снижения размерности."
      ],
      "metadata": {
        "id": "ehettf5U-tYT"
      },
      "id": "ehettf5U-tYT"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def evaluate_knn(X, y, k=3):\n",
        "    \"\"\"\n",
        "    Простой классификатор k-NN с кросс-валидацией Leave-One-Out.\n",
        "\n",
        "    Вход:\n",
        "        X — матрица признаков (список списков)\n",
        "        y — список меток классов\n",
        "        k — количество ближайших соседей для голосования\n",
        "\n",
        "    Выход:\n",
        "        Возвращает точность классификации (доля правильных ответов)\n",
        "    \"\"\"\n",
        "\n",
        "    correct = 0\n",
        "    n = len(X)\n",
        "\n",
        "    for i in range(n):\n",
        "        xi = X[i]\n",
        "        yi = y[i]\n",
        "\n",
        "        distances = []\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                continue\n",
        "            d = sum((xi[d] - X[j][d]) ** 2 for d in range(len(xi))) ** 0.5\n",
        "            distances.append((d, y[j]))\n",
        "\n",
        "        distances.sort()\n",
        "        neighbors = distances[:k]\n",
        "        votes = {}\n",
        "        for _, label in neighbors:\n",
        "            votes[label] = votes.get(label, 0) + 1\n",
        "\n",
        "        predicted = max(votes.items(), key=lambda x: x[1])[0]\n",
        "        if predicted == yi:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / n\n",
        "\n",
        "def apply_pca_to_dataset_builtin(k: int = 2, auto: bool = False, threshold: float = 0.95):\n",
        "    \"\"\"\n",
        "    Применяет PCA к встроенному датасету iris и сравнивает точность классификации до и после.\n",
        "    \"\"\"\n",
        "    dataset = load_iris()\n",
        "    X = dataset.data.tolist()\n",
        "    y = dataset.target.tolist()\n",
        "\n",
        "    acc_before = evaluate_knn(X, y)\n",
        "\n",
        "    if auto:\n",
        "        X_proj, _ = pca_new(X, auto=True, threshold=threshold)\n",
        "    else:\n",
        "        X_proj, _ = pca_new(X, k=k)\n",
        "\n",
        "    acc_after = evaluate_knn(X_proj, y)\n",
        "\n",
        "    return X_proj, acc_before, acc_after\n",
        "X_proj, acc1, acc2 = apply_pca_to_dataset_builtin(k=2)\n",
        "\n",
        "print(f\"Точность до PCA: {acc1:.2%}\")\n",
        "print(f\"Точность после PCA: {acc2:.2%}\")\n"
      ],
      "metadata": {
        "id": "6Q7wJrIpaj_p"
      },
      "id": "6Q7wJrIpaj_p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Вывод**"
      ],
      "metadata": {
        "id": "V7zdhXPJ4kOn"
      },
      "id": "V7zdhXPJ4kOn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "В процессе выполнения этой лабораторной работы мы не только познакомилась с теорией метода главных компонент, но и реализовали его на практике.\n",
        "\n",
        "Особенно интересным оказалось наблюдать, как данные \"сжимаются\" без значительной потери информации. Визуализация результатов на графиках помогла увидеть, как PCA упрощает структуру данных, делая их более понятными.​\n",
        "\n",
        "Были и трудности. Например, реализация алгоритма поиска собственных значений потребовала много времени и усилий. Но именно преодоление этих сложностей принесло настоящее удовлетворение и чувство гордости за проделанную работу.​\n",
        "\n",
        "Хотим выразить благодарность практикам и преподавателям за такую интересную и полезную лабораторную работу. Она не только углубила наши знания в области анализа данных, но и показала, как теоретические методы применяются на практике. Теперь мы чувствуем себя более уверенно в работе с многомерными данными и понимаем, как важно уметь их анализировать и визуализировать.​\n",
        "\n"
      ],
      "metadata": {
        "id": "15WCAodu4-32"
      },
      "id": "15WCAodu4-32"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}